<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Apache Spark 2 Workshop :: Basic Aggregation (Spark SQL)</title>

    <meta name="description" content="Apache Spark 2 Workshop :: Basic Aggregation (Spark SQL)">
    <meta name="author" content="Jacek Laskowski">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/beige.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!-- https://github.com/hakimel/reveal.js/issues/174 -->
    <style>
      .slides .header {
        position: absolute;
        top: 0px;
        right: 0px;
      }
      .slides .footer {
        position: absolute;
        bottom: 0px;
        left: 35%;
      }
    </style>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <div class="footer">
          <small>Copyright ©2017 Jacek Laskowski</small>
        </div>

        <section class="intro" data-transition="zoom">
          <p>
            <img width="5%" style="background:none; border:none; box-shadow:none;" data-src="images/scala-logo.png">
            <img width="17%" style="background:none; border:none; box-shadow:none;" data-src="images/spark-logo.png">
            <img width="8%" src="images/jacek_laskowski_20141201_512px.png" style="border: 0">
          </p>
          <h1>Basic Aggregation</h1>
          <h3>(agg, groupBy and groupByKey)</h3>
          <h2>Apache Spark 2 / Spark SQL</h2>
          <h4><a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> / <a href="http://stackoverflow.com/users/1305344/jacek-laskowski">StackOverflow</a> / <a href="https://github.com/jaceklaskowski">GitHub</a><br />
          Books: <a href="http://bit.ly/mastering-apache-spark">Mastering Apache Spark</a>  / <a href="http://bit.ly/spark-structured-streaming">Spark Structured Streaming</a></h4>
        </section>

        <section id="agenda" data-markdown>
          <script type="text/template">
            ## Agenda

            1. [Aggregate Functions](#/aggregate-functions)
            1. [agg Operator](#/agg-operator)
              * [Exercise](#/agg-operator-exercise): Finding maximum value (agg)
            1. [Untyped groupBy Operator](#/groupBy-operator)
              * [Exercise](#/groupBy-operator-exercise-max): Finding maximum values per group (groupBy)
              * [Exercise](#/groupBy-operator-exercise-collect): Collect values (per group)
              * [Exercise](#/exercise-multiple-aggregations): Multiple Aggregations
            1. [Typed groupByKey Operator](#/groupByKey-operator)
            1. ["Explaining" Query Plans](#/explain)
          </script>
        </section>

        <section id="aggregate-functions" data-markdown>
          <script type="text/template">
            ## Aggregate Functions

            1. **Aggregate functions** accept a group of records as input
              * Unlike regular functions that act on a single record
            1. Available among standard functions
              ```scala
              import org.apache.spark.sql.functions._
              ```
            1. _Usual suspects_: **avg**, **collect_list**, **count**, **min**, **mean**, **sum**
            1. You can create custom **user-defined aggregate functions (UDAFs)**
            1. Read [functions object's scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)
          </script>
        </section>

        <section>
          <section id="agg-operator" data-markdown>
            <script type="text/template">
              ## agg Operator

              1. **agg** applies an aggregate function to records in Dataset
              ```scala
                val ds = spark.range(10)
                ds.agg(sum('id) as "sum")
              ```
              1. Entire Dataset acts as a single group
                * **groupBy** used to define groups (_on the following slide_)
              1. Creates DataFrame
                * &hellip;hence considered untyped due to **Row** inside
                * Typed variant available (_on the following slide_)
              1. Switch to Mastering Apache Spark 2
                * [Basic Aggregation &mdash; Typed and Untyped Grouping Operators](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-basic-aggregation.html)
            </script>
          </section>
          <section id="agg-operator-exercise" data-markdown>
            <script type="text/template">
              ## Exercise: Finding maximum value (agg)

              1. Develop a standalone Spark SQL application (IntelliJ IDEA)
              1. Use **max** function (with **agg** or **select** operators)
              1. Read [Spark API scaladoc for function object](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) to learn about **max** function
              1. Time: **15 mins**
            </script>
          </section>
        </section>

        <section>
          <section id="groupBy-operator" data-markdown>
            <script type="text/template">
              ## Untyped groupBy Operator

              1. **groupBy** groups records in Dataset per _discriminator function_
                ```
                val nums = spark.range(10)
                nums.groupBy('id % 2 as "group").agg(sum('id) as "sum")
                ```
              1. Creates **RelationalGroupedDataset**
                * Supports untyped, Row-based **agg**
                * Shortcuts for _the usual suspects_, e.g. **avg**, **count**, **pivot**
                * Read [RelationalGroupedDataset's scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
            </script>
          </section>
          <section id="groupBy-operator-exercise-max" data-markdown>
            <script type="text/template">
              ## Exercise: Finding maximum values per group (groupBy)

              1. Develop a standalone Spark SQL application (IntelliJ IDEA)
              1. Use **groupBy** operator with **max** function
                * Use **spark.range** for sample numbers
                * Use a CSV file with two columns for a real dataset
              1. Read [Spark API scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html)
              1. Time: **30 mins**
            </script>
          </section>
          <section id="groupBy-operator-exercise-collect" data-markdown>
            <script type="text/template">
              ## Exercise: Collect values (per group)

              1. Collect values per group in single column
              1. Read Spark API scaladoc for [functions object](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) to find **the function**
              1. Time: **30 mins**
            </script>
          </section>
          <section id="exercise-multiple-aggregations" data-markdown>
            <script type="text/template">
              ## Exercise: Multiple Aggregations

              1. Compute **max** and **min** for groups
                * **TIP**: Review available **agg** operators
              1. Read [the scaladoc of Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
              1. Time: **30 mins**
            </script>
          </section>
        </section>

        <section id="groupByKey-operator" data-markdown>
          <script type="text/template">
            ## Typed groupByKey Operator

            1. **groupByKey** similar to **groupBy** operator, but gives typed interface
            ```scala
              ds.groupByKey(_ % 2).reduceGroups(_ + _).show

              // compare to untyped query
              nums.groupBy('id % 2 as "group").agg(sum('id) as "sum")
            ```
            1. Creates **KeyValueGroupedDataset**
              * Supports typed **agg**
              * Shortcuts for _the usual suspects_, e.g. **reduceGroups**, **mapValues**, **mapGroups**, **flatMapGroups**, **cogroup**
              * Read [KeyValueGroupedDataset's scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
          </script>
        </section>

        <section id="explain">
          <h2>"Explaining" Query Plans</h2>
          <ol>
            <li><b>explain</b> is an operator that shows the logical and physical plans
              <ul>
                <li><b>EXPLAIN [EXTENDED]</b> in SQL</li>
              </ul>
            </li>
            <li><b>Logical Query Plan Analyzer</b> is a RuleExecutor with rules to optimize query execution</li>
            <li>Switch to Mastering Apache Spark 2
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-dataset-operators.html#explain">Explaining Logical and Physical Plans &mdash; explain Operator</a></li>
                <li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-Analyzer.html">Analyzer &mdash; Logical Query Plan Analyzer</a></li>
              </ul>
            </li>
          </ol>
        </section>

        <section id="recap" data-markdown>
          <script type="text/template">
            ## Recap

            1. [Aggregate Functions](#/aggregate-functions)
            1. [agg Operator](#/agg-operator)
              * [Exercise](#/agg-operator-exercise): Finding maximum value (agg)
            1. [Untyped groupBy Operator](#/groupBy-operator)
              * [Exercise](#/groupBy-operator-exercise-max): Finding maximum values per group (groupBy)
              * [Exercise](#/groupBy-operator-exercise-collect): Collect values (per group)
              * [Exercise](#/exercise-multiple-aggregations): Multiple Aggregations
            1. [Typed groupByKey Operator](#/groupByKey-operator)
            1. ["Explaining" Query Plans](#/explain)
          </script>
        </section>

        <section id="questions" style="text-align: left" data-markdown>
          <script type="text/template">
            # Questions?

              * Read [Mastering Apache Spark 2](https://bit.ly/mastering-apache-spark)
                * [https://bit.ly/mastering-apache-spark](https://bit.ly/mastering-apache-spark)
              * Follow [@jaceklaskowski](https://twitter.com/jaceklaskowski) on twitter
              * Upvote [my questions and answers on StackOverflow](http://stackoverflow.com/users/1305344/jacek-laskowski)
              * Use [Jacek's code at GitHub](https://github.com/jaceklaskowski)
              * Read [blog posts on Medium](https://medium.com/@jaceklaskowski)
              * Upvote [my answers on Quora](https://www.quora.com/profile/Jacek-Laskowski)
              * Connect on [LinkedIn](https://www.linkedin.com/in/jaceklaskowski/)
              * Visit [Jacek Laskowski's blog](https://blog.jaceklaskowski.pl)
          </script>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>

        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,

            transition: 'slide', // none/fade/slide/convex/concave/zoom

            // Optional reveal.js plugins
            dependencies: [
                { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                { src: 'plugin/zoom-js/zoom.js', async: true },
                { src: 'plugin/notes/notes.js', async: true }
            ]
        });

    </script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-45999426-3', 'auto');
      ga('send', 'pageview');

    </script>
  </body>
</html>
