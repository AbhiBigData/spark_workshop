<!doctype html>
<!-- Inspired by view-source:https://brookewenig.github.io/SparkOverview.html  -->
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <meta name="description" content="Apache Spark 2 Workshop | Columns, Operators, Standard Functions and UDFs">
    <meta name="author" content="Jacek Laskowski">

    <title>Apache Spark 2 Workshop | Columns, Operators, Standard Functions and UDFs</title>

    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="reveal.js/css/jacek.css">
    <link rel="stylesheet" href="reveal.js/css/theme/beige.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section class="intro" data-transition="zoom" id="home">
          <span class="menu-title" style="display: none">Home</span>
          <p>
            <img width="12%" style="background:none; border:none; box-shadow:none;" data-src="images/spark-logo.png">
            <img width="6%" src="images/jacek_laskowski_20141201_512px.png" style="border: 0">
          </p>
          <h1 style="font-size: 2.57em;">Columns, Operators,</h1>
          <h1 style="font-size: 2.57em;">Standard and User-Defined Functions</h1>

          <h4><a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> / <a href="http://stackoverflow.com/users/1305344/jacek-laskowski">StackOverflow</a> / <a href="https://github.com/jaceklaskowski">GitHub</a><br />
          Books: <a href="http://bit.ly/mastering-apache-spark">Mastering Apache Spark</a>  / <a href="http://bit.ly/spark-structured-streaming">Spark Structured Streaming</a></h4>

          <footer style="font-size: small;">&copy;<a href="https://medium.com/@jaceklaskowski">Jacek Laskowski</a> 2017 | @jaceklaskowski | jacek@japila.pl</footer>
        </section>

        <section id="agenda" data-markdown>
          <script type="text/template">
            <!-- .slide: style="font-size: 90%" -->
            ## Agenda

            1. [Dataset Columns](#/columns)
            1. [Column Operators / Expressions](#/column-operators)
            1. [Dataset Operators](#/dataset-operators)
              * [Exercise](#/dataset-operators-exercise-createOrReplaceTempView): Using createOrReplaceTempView and SQL
              * [Exercise](#/dataset-operators-exercise-flatMap) (hard): Using flatMap
            1. [Standard Functions](#/standard-functions)
              * [Exercise](#/standard-functions-exercise): Using Standard Functions (upper)
            1. [Standard Functions for Date and Time](#/standard-functions-datetime)
              * [Exercise](#/exercise-datetime-functions): Difference in Days Between Date Strings
            1. [User-Defined Functions (UDFs)](#/udf-heads-up)
              * [Exercise](#/udf-exercise): Using UDFs
          </script>
        </section>

        <section id="columns" data-markdown>
          <script type="text/template">
            <!-- .slide: style="font-size: 90%" -->
            ## Dataset Columns

            1. **Column** is a function that generates a value per row
            1. **Column** type with methods
              * _Explained in the following slide_
            1. Columns can be <b>free</b> or <b>bound</b> (associated or not with Datasets)
              ```scala
              $"columnName"   // free (no Dataset) column reference
              myDataset("id") // bound column reference
              ```
            1. Use **import spark.implicits._**
            1. Switch to Mastering Apache Spark 2
              * [Dataset Columns](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-Column.html)
          </script>
        </section>

        <section id="column-operators" data-markdown>
          <script type="text/template">
            <!-- .slide: style="font-size: 90%" -->
            ## Column Operators / Expressions

            1. **Column** is an Catalyst expression to generate values
            1. Special **&#9733;** (star) column reference
            1. **Operators** to create (compound) columns
              * **as**, **alias** or **name** for aliases
              * **===** for equality _(!)_
              * **desc**, **desc_nulls_first** and **desc_nulls_last** (and for **asc**)
              * **getItem** to access items in arrays and maps
              * **over** for windowed aggregates
              * **cast** for casting to a custom data type
              * **when** and **otherwise** for conditional values
            1. **Home exercise**: Read up [Column's scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column)
          </script>
        </section>

        <section>
          <section id="dataset-operators" data-markdown>
            <script type="text/template">
              <!-- .slide: style="font-size: 90%" -->
              ## Dataset Operators

              1. **Dataset Operators** work with all records of a Dataset
                * **as** to converting a Row-based DataFrame to a Dataset
                * **createOrReplaceTempView** to register a temporary view
                * **explain** to show the logical and execution plans
                * **flatMap** to "explode" records
                * **randomSplit** to split records to two Datasets randomly
                * **select**, **selectExpr**, **filter**, **where**
                * &hellip; _many many more_ &mdash; read up [Dataset's scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
              1. Switch to Mastering Apache Spark 2
                * [Dataset Operators](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-dataset-operators.html)
            </script>
          </section>
          <section id="dataset-operators-exercise-createOrReplaceTempView" data-markdown>
            <script type="text/template">
              ## Exercise: Using createOrReplaceTempView and SQL

              1. Use **createOrReplaceTempView** to register a Dataset and execute a SQL (_of your liking_)
                * Read up [the scaladoc of createOrReplaceTempView](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
              1. Time: **15 mins**
            </script>
          </section>
          <section id="dataset-operators-exercise-flatMap" data-markdown>
            <script type="text/template">
              <!-- .slide: style="font-size: 90%" -->
              ## Exercise (hard): Using flatMap

              1. Create a Dataset with a column of array type
                ```scala
                val nums = Seq(Seq(1,2,3)).toDF("nums")
                scala> nums.printSchema
                root
                 |-- nums: array (nullable = true)
                 |    |-- element: integer (containsNull = false)
                ```
              1. Use **flatMap** to expand the array column
                * Read up [flatMap's scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
              1. (extra) Compare performance of **flatMap** and **explode**
              1. Time: **30 mins**
            </script>
          </section>
        </section>

        <section>
          <section id="standard-functions" data-markdown>
            <script type="text/template">
              <!-- .slide: style="font-size: 90%" -->
              ## Standard Functions <small>(1 of 2)</small>

              1. <!-- .element: style="font-size: 0.9em;" --> **Standard functions** (aka **native functions**) are built-in functions (in **SparkSession**) to transform columns into a column
                * Aggregate functions, e.g. **avg**, **count**, **sum**
                * Collection functions, e.g. **explode**, **from_json**
                * Date time functions, e.g. **current_timestamp**, **to_date**, **window**
                  * <!-- .element: style="font-size: 0.8em;" --> _More in the upcoming [slide](#/standard-functions-datetime)_
                * Math functions, e.g. **conv**, **factorial**, **pow**
                * Non-aggregate functions, e.g. **array**, **broadcast**, **expr**, **lit**
                * Sorting functions, e.g. **asc**, **asc_nulls_first**, **asc_nulls_last**
                * String functions, e.g. **concat_ws**, **trim**, **upper**
                * UDF functions, e.g. **callUDF**
                * Window functions, e.g. **rank**, **row_number**
            </script>
          </section>
          <section id="standard-functions-2" data-markdown>
            <script type="text/template">
              ## Standard Functions <small>(2 of 2)</small>

              1. Import **org.apache.spark.sql.functions** object
              ```scala
              import org.apache.spark.sql.functions._
              ```
              1. Switch to Mastering Apache Spark 2
                * [Standard Functions &mdash; functions object](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-functions.html)
              1. **Home exercise**: Read up [functions object's scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)
            </script>
          </section>
          <section id="standard-functions-exercise" data-markdown>
            <script type="text/template">
              <!-- .slide: style="font-size: 90%" -->
              ## Exercise: Using Standard Functions (upper)

              1. Develop a standalone Spark SQL application (IntelliJ IDEA)
              1. Use **upper** function with **withColumn** or **select** operators
                * Think about the difference between the Dataset operators
              1. Use Spark API scaladoc for [functions object](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)
              1. (extra) Load a data set from CSVs given on command line
              1. Time: **30 mins**
            </script>
          </section>
        </section>

        <section>
          <section id="standard-functions-datetime" data-markdown>
            <script type="text/template">
              <!-- .slide: style="font-size: 90%" -->
              ## Standard Functions for Date and Time

              1. **unix_timestamp**
              1. **to_timestamp**
              1. **window**
              1. Read Spark API scaladoc for [functions object](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)
              1. Switch to Mastering Apache Spark 2
                * [Standard Functions for Date and Time](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-functions-datetime.html)
            </script>
          </section>
          <section id="exercise-datetime-functions" data-markdown>
            <script type="text/template">
              ## Exercise: Difference in Days Between Dates As Strings <small>(1 of 2)</small>

              1. Given dataset with date strings calculate number of days between them and current day
              1. Use **unix_timestamp** function and **cast** to **timestamp**
              1. Use **to_date** and **datediff** functions
              1. Read Spark API scaladoc for [functions object](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)
            </script>
          </section>
          <section id="exercise-datetime-functions-setup" data-markdown>
            <script type="text/template">
              ## Exercise: Difference in Days Between Dates As Strings <small>(2 of 2)</small>

```scala
val dates = Seq(
   "08/11/2015",
   "09/11/2015",
   "09/12/2015").toDF("date_string")

val diffs = // <-- your solution here

scala> diffs.show
+-----------+-------------------+----+
|date_string|          timestamp|diff|
+-----------+-------------------+----+
| 08/11/2015|2015-11-08 00:00:00| 584|
| 09/11/2015|2015-11-09 00:00:00| 583|
| 09/12/2015|2015-12-09 00:00:00| 553|
+-----------+-------------------+----+
```
            </script>
          </section>
        </section>

        <section>
          <section id="udf-heads-up" data-markdown>
            ## User-Defined Functions (UDFs)
            ### « HEADS-UP »

            > **Use the standard functions whenever possible** before reverting to using your own custom UDFs.

            > UDFs are a blackbox for Spark and so it does **not even try** to optimize them.
          </section>
          <section id="user-defined-functions" data-markdown>
            <script type="text/template">
              <!-- .slide: style="font-size: 85%" -->
              ## UDFs &mdash; User-Defined Functions

              1. User-Defined Function extends the "vocabulary" of Spark SQL
              1. Use <b>udf</b> function to define a user-defined function
                ```scala
                // pure Scala function
                val myUpperFn = (input: String) => input.toUpperCase

                // user-defined function
                val myUpper = udf(myUpperFn)
                ```
              1. Use UDFs as standard functions
                * <b>withColumn</b>, <b>select</b>, <b>filter</b> etc.
                * Also **callUDF** function
              1. Switch to Mastering Apache Spark 2
                * [UDFs — User-Defined Functions](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html)
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              <!-- .slide: style="font-size: 90%" -->
              ## Registering UDFs for SQL queries

              1. Use <b>spark.udf.register</b> to register a Scala function as a user-defined function
                ```scala
                // pure Scala function
                val myUpperFn = (input: String) => input.toUpperCase

                // register Scala function as UDF
                spark.udf.register("myUpper", myUpperFn)

                // Use myUpper as if it were a standard function
                sql("select myUpper(name) from people").show
                ```
            </script>
          </section>
          <section id="udf-exercise" data-markdown>
            <script type="text/template">
              ## Exercise: Using UDFs

              1. Develop a standalone Spark SQL application (IntelliJ IDEA)
              1. Roll your own **upper** function
                * Use Scala's [StringOps.toUpperCase](http://www.scala-lang.org/api/2.11.8/index.html#scala.collection.immutable.StringOps)
              1. **Explain** the logical and execution plans
                * Think what happens for UDFs with **if** conditions
              1. (extra) Use the UDF in SQL, i.e. **spark.sql**
              1. (extra) Use **callUDF**
              1. Time: **30 mins**
            </script>
          </section>
        </section>

        <section id="recap" data-markdown>
          <script type="text/template">
            <!-- .slide: style="font-size: 90%" -->
            ## Recap

            1. [Dataset Columns](#/columns)
            1. [Column Operators / Expressions](#/column-operators)
            1. [Dataset Operators](#/dataset-operators)
              * [Exercise](#/dataset-operators-exercise-createOrReplaceTempView): Using createOrReplaceTempView and SQL
              * [Exercise](#/dataset-operators-exercise-flatMap) (hard): Using flatMap
            1. [Standard Functions](#/standard-functions)
              * [Exercise](#/standard-functions-exercise): Using Standard Functions (upper)
            1. [Standard Functions for Date and Time](#/standard-functions-datetime)
              * [Exercise](#/exercise-datetime-functions): Difference in Days Between Date Strings
            1. [User-Defined Functions (UDFs)](#/udf-heads-up)
              * [Exercise](#/udf-exercise): Using UDFs
          </script>
        </section>

        <section style="text-align: left" data-markdown id="questions">
          <script type="text/template">
            # Questions?

            * Read [Mastering Apache Spark 2](https://bit.ly/mastering-apache-spark)
              * [https://bit.ly/mastering-apache-spark](https://bit.ly/mastering-apache-spark)
            * Follow [@jaceklaskowski](https://twitter.com/jaceklaskowski) on twitter
            * Upvote [my questions and answers on StackOverflow](http://stackoverflow.com/users/1305344/jacek-laskowski)
          </script>
        </section>

      </div>
    </div>

    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.js"></script>

    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        slideNumber: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        menu: {
          markers: true,
          openSlideNumber: true
        },
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/markdown/marked.js' },
          { src: 'reveal.js/plugin/markdown/markdown.js' },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true },
          { src: 'reveal.js/plugin/menu/menu.js', async: true },
          { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
  </body>
</html>
